{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# languange processing imports\n",
    "import nltk\n",
    "import string\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pprint import pprint\n",
    "# model imports\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import CoherenceModel, Phrases, phrases\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read tables from mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printable = set(string.printable)\n",
    "s = 'Ti\\u00ebsto'\n",
    "list(filter(lambda x: x in printable, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lyrics.csv\n",
    "data = pd.read_csv('lyrics.csv', names=['singer', 'song', 'lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        return simple_preprocess(text)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lyrics['singer'] = list(map(simple_preprocess, lyrics.singer))\n",
    "#lyrics['song'] = list(map(simple_preprocess, lyrics.singer))\n",
    "data['lyrics'] = list(map(preprocess, data.lyrics.values))\n",
    "data = data[~data['lyrics'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constrcut doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_w2v_features(d2v_model, sentence):\n",
    "    #vector = d2v_model.docvecs[i]\n",
    "        \n",
    "    inferred_vector = d2v_model.infer_vector(sentence)\n",
    "    return inferred_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "train_corpus = list(map(lambda i:TaggedDocument(data.iloc[i].lyrics,['doc%d' % i]), range(0,data.shape[0])))\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, vector_size=100, sample=1e-4, negative=5, workers=8, epochs=200)\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "#train_data3['w2v_features'] = list(map(lambda i:\n",
    "#                                      get_w2v_features(model, i),\n",
    "#                                      range(0, train_data3.shape[0])))\n",
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "\n",
    "model.save(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6872, 1: 3})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index('doc%d' % doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "import collections\n",
    "collections.Counter(ranks) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6872, 1: 3})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('store_ranks.npy', 'wb') as writer:\n",
    "    pickle.dump(ranks, writer)\n",
    "    \n",
    "with open('store_ranks.npy', 'rb') as reader:\n",
    "    ranks3 = pickle.load(reader)\n",
    "collections.Counter(ranks3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
    "train_data3['w2v_features'] = list(map(lambda sentence:\n",
    "                                      infer_w2v_features(model, sentence),\n",
    "                                      train_data3.tokenized_sentences.values))\n",
    "test_data3['w2v_features'] = list(map(lambda sentence:\n",
    "                                      infer_w2v_features(model, sentence),\n",
    "                                      test_data3.tokenized_sentences.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.concatenate((train_data3.id.values, test_data3.id.values))\n",
    "columns = test_data3.columns.values[10:-2]\n",
    "reviewers_semantic_sim = pd.DataFrame(index=index, columns=columns)\n",
    "dataset = pd.concat([train_data3, test_data3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, pr in dataset.iterrows():\n",
    "    for reviewer in train_data3.columns.values[10:-2]:\n",
    "        train_data = dataset.loc[:index][:-1]\n",
    "        reviewed_prs = train_data[train_data[reviewer]==1]\n",
    "        if reviewed_prs.shape[0] == 0:\n",
    "            reviewers_semantic_sim.loc[pr.id, reviewer] = [-1]\n",
    "            continue\n",
    "        list_vectors = list(reviewed_prs.w2v_features.values)\n",
    "        ##vector_features = normalize(pr.w2v_features[:,np.newaxis], axis=0).ravel() \n",
    "        ##similar_scores = [np.dot(normalize(pr_vector[:,np.newaxis], axis=0).ravel(), vector_features) for pr_vector in reviewers_vectors[reviewer]]\n",
    "        similar_scores = [np.dot(pr_vector, pr.w2v_features)/(np.linalg.norm(pr_vector)*np.linalg.norm(pr.w2v_features)) for pr_vector in list_vectors]\n",
    "        ##if max(similar_scores) > 0.6:\n",
    "        ##    print(reviewer)\n",
    "        ##    print(pr.lda_text)\n",
    "        ##    print(pr.id)\n",
    "        ##    print(similar_scores.index(max(similar_scores)))\n",
    "        ##    print((max(similar_scores)))\n",
    "        ##    print(len(similar_scores))\n",
    "        ##    print(len(reviewers_vectors[reviewer]))\n",
    "        ##    print(reviewed_prs.iloc[similar_scores.index(max(similar_scores))].w2v_text)\n",
    "        ##    print(reviewed_prs.iloc[similar_scores.index(max(similar_scores))].id)\n",
    "        reviewers_semantic_sim.loc[pr.id, reviewer] = similar_scores\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 8602})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(reviewers_semantic_sim['rchande'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, pr in test_data3.iterrows():\n",
    "    for reviewer in test_data3.columns.values[10:-2]:\n",
    "        reviewed_prs = train_data3[train_data3[reviewer]==1]\n",
    "        list_vectors = list(reviewed_prs.w2v_features.values)\n",
    "        #vector_features = normalize(pr.w2v_features[:,np.newaxis], axis=0).ravel() \n",
    "        #similar_scores = [np.dot(normalize(pr_vector[:,np.newaxis], axis=0).ravel(), vector_features) for pr_vector in reviewers_vectors[reviewer]]\n",
    "        similar_scores = [np.dot(pr_vector, pr.w2v_features)/(np.linalg.norm(pr_vector)*np.linalg.norm(pr.w2v_features)) for pr_vector in list_vectors]\n",
    "        #if max(similar_scores) > 0.6:\n",
    "        #    print(reviewer)\n",
    "        #    print(pr.lda_text)\n",
    "        #    print(pr.id)\n",
    "        #    print(similar_scores.index(max(similar_scores)))\n",
    "        #    print((max(similar_scores)))\n",
    "        #    print(len(similar_scores))\n",
    "        #    print(len(reviewers_vectors[reviewer]))\n",
    "        #    print(reviewed_prs.iloc[similar_scores.index(max(similar_scores))].w2v_text)\n",
    "        #    print(reviewed_prs.iloc[similar_scores.index(max(similar_scores))].id)\n",
    "        reviewers_semantic_sim.loc[pr.id, reviewer] = similar_scores\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 54)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewers_semantic_sim[reviewers_semantic_sim['rchande'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MattGertz', 'jcouv', 'jaredpar', 'mavasani',\n",
       "       'VladimirReshetnikov', 'shyamnamboodiripad', 'dpoeschl',\n",
       "       'ManishJayaswal', 'gafter', 'ghost', 'tannergooding', 'mattwar',\n",
       "       'khyperia', 'paulvanbrenk', 'jmarolf', 'TyOverby', 'alrz',\n",
       "       'AnthonyDGreen', 'sharwell', 'msJohnHamby', 'natidea', 'mmitche',\n",
       "       'dotnet-bot', 'OmarTawfik', 'balajikris', 'KirillOsenkov',\n",
       "       'MeiChin-Tsai', 'tmeschter', 'AArnott', 'panopticoncentral',\n",
       "       'VSadov', 'dnfclas', 'amcasey', 'ivanbasov', 'tmat', 'stephentoub',\n",
       "       'jinujoseph', 'KevinH-MS', 'agocke', 'genlu', 'srivatsn',\n",
       "       'AlekseyTs', 'jasonmalinowski', 'DustinCampbell', 'rchande',\n",
       "       'CyrusNajmabadi', 'AdamSpeight2008', 'brettfo', 'Pilchie',\n",
       "       'davkean', 'cston', 'nguerrera', 'heejaechang', '333fred'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 2373, True: 6229})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(reviewers_semantic_sim_max['rchande'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.concatenate((train_data3.id.values, test_data3.id.values))\n",
    "reviewers_semantic_sim_max = pd.DataFrame(index=index, columns=columns)\n",
    "for row in index:\n",
    "    for column in columns:\n",
    "        similar_scores = reviewers_semantic_sim.loc[row, column]\n",
    "        reviewers_semantic_sim_max.loc[row, column] = max(similar_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers_semantic_sim_sum = pd.DataFrame(index=index, columns=columns)\n",
    "for row in index:\n",
    "    for column in columns:\n",
    "        similar_scores = reviewers_semantic_sim.loc[row, column]\n",
    "        reviewers_semantic_sim_sum.loc[row, column] = sum(similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers_semantic_sim_average = pd.DataFrame(index=index, columns=columns)\n",
    "for row in index:\n",
    "    for column in columns:\n",
    "        similar_scores = reviewers_semantic_sim.loc[row, column]\n",
    "        reviewers_semantic_sim_average.loc[row, column] = sum(similar_scores)/len(similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers_semantic_sim_min = pd.DataFrame(index=index, columns=columns)\n",
    "for row in index:\n",
    "    for column in columns:\n",
    "        similar_scores = reviewers_semantic_sim.loc[row, column]\n",
    "        reviewers_semantic_sim_min.loc[row, column] = min(similar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewers_semantic_sim_max.to_csv('reviewers_semantic_sim_max.csv')\n",
    "reviewers_semantic_sim_min.to_csv('reviewers_semantic_sim_min.csv')\n",
    "reviewers_semantic_sim_sum.to_csv('reviewers_semantic_sim_sum.csv')\n",
    "reviewers_semantic_sim_average.to_csv('reviewers_semantic_sim_average.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_array(array, filename):\n",
    "    base_dir = 'D:\\\\reviewer recommendation\\\\LDA&w2v\\\\'\n",
    "    np.save(os.path.join(base_dir, filename), array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct train and test dataset\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "features_train = train_data3[['w2v_features']]\n",
    "features_train_w2v = np.array(list(map(np.array, features_train.w2v_features)))\n",
    "\n",
    "save_array(features_train_w2v, 'train_data.npy')\n",
    "\n",
    "label_train = np.array(train_data3.loc[:, first_reviewer: last_reviewer])\n",
    "save_array(label_train, 'train_label.npy')\n",
    "\n",
    "\n",
    "features_test = test_data3[['w2v_features']]\n",
    "features_test_w2v = np.array(list(map(np.array, features_test.w2v_features)))\n",
    "\n",
    "save_array(features_test_w2v, 'test_data.npy')\n",
    "\n",
    "label_test = np.array(test_data3.loc[:, first_reviewer: last_reviewer])\n",
    "save_array(label_test, 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_test.shape)\n",
    "test_data3.shape\n",
    "\n",
    "label_test.sum(axis=0)==0\n",
    "\n",
    "print(list(label_test.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#parameters = {'activation':['logistic','tanh'],\n",
    "#              'solver': ['sgd','adam'], \n",
    "#              'max_iter': [1000,2000,3000],\n",
    "#              'alpha': 10.0 ** -np.arange(3, 7), \n",
    "#              'hidden_layer_sizes':[(128,64,32),(128,64)],\n",
    "#              #'random_state':[0,1,2,3,4,5,6,7,8,9]\n",
    "#             }\n",
    "#\n",
    "mlp = MLPClassifier(activation='logistic', solver='sgd', max_iter=10000, alpha=1e-03, hidden_layer_sizes=(8,))\n",
    "mlp.fit(features_train_w2v, label_train)\n",
    "# clf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlp.loss_curve_)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(mlp.loss_curve_)\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('score_test.npy',mlp.predict_proba(features_test_w2v))\n",
    "np.save('df_probs_train.npy',mlp.predict_proba(features_train_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp = OneVsRestClassifier(MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(32,16), max_iter = 100, random_state=1))\n",
    "mlp.fit(features_train_w2v, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators':[10, 50, 100, 500, 1000],\n",
    "              'max_depth':[None, 3, 5, 10],\n",
    "              'min_samples_split':[2,5,10]\n",
    "             }\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, cv=3, scoring='precision_macro')\n",
    "clf.fit(features_train_w2v, label_train)\n",
    "print(clf.score(features_train_w2v, label_train))\n",
    "print(clf.best_params_)\n",
    "#model = OneVsRestClassifier(RandomForestClassifier(n_estimators=10000, max_depth=5, random_state=0, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "#print()\n",
    "print(clf.best_params_)\n",
    "#print()\n",
    "print(\"Grid scores on development set:\")\n",
    "#print()\n",
    "means = clf.cv_results_['mean_train_score']\n",
    "stds = clf.cv_results_['std_train_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs = clf.predict_proba(features_test_w2v)\n",
    "np.save('score_test.npy',np.array(df_probs))\n",
    "print(df_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "vector_size = 150\n",
    "train_corpus = list(map(lambda i:TaggedDocument(train_data3.iloc[i].tokenized_sentences,['doc%d' % i]), range(0,train_data3.shape[0])))\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=vector_size, sample=1e-4, negative=5, workers=8, epochs=200)\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "fname = get_tmpfile(\"D:\\\\reviewer recommendation\\\\LDA&w2v\\\\d2v_models\\\\my_doc2vec_model%s\" % vector_size)\n",
    "\n",
    "model.save(fname)\n",
    "print('finish d2v model')\n",
    "\n",
    "\n",
    "ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index('doc%d' % doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "#import collections\n",
    "print(collections.Counter(ranks))\n",
    "    \n",
    "\n",
    "import pickle\n",
    "with open('D:\\\\reviewer recommendation\\\\LDA&w2v\\\\store_ranks\\\\store_ranks%s.npy' % vector_size, 'wb') as writer:\n",
    "    pickle.dump(ranks, writer)\n",
    "    \n",
    "#with open('store_ranks.npy', 'rb') as reader:\n",
    "#    ranks3 = pickle.load(reader)\n",
    "#collections.Counter(ranks3)\n",
    "\n",
    "#model = Doc2Vec.load(fname)  # you can continue training with the loaded model!\n",
    "train_data3['w2v_features'] = list(map(lambda sentence:\n",
    "                                      infer_w2v_features(model, sentence),\n",
    "                                      train_data3.tokenized_sentences.values))\n",
    "test_data3['w2v_features'] = list(map(lambda sentence:\n",
    "                                      infer_w2v_features(model, sentence),\n",
    "                                      test_data3.tokenized_sentences.values))\n",
    "\n",
    "print('finish w2v features')\n",
    "# conduct train and test dataset\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "features_train = train_data3[['w2v_features']]\n",
    "features_train_w2v = np.array(list(map(np.array, features_train.w2v_features)))\n",
    "\n",
    "\n",
    "label_train = np.array(train_data3.loc[:, first_reviewer: last_reviewer])\n",
    "save_array(label_train, 'train_label.npy')\n",
    "\n",
    "\n",
    "features_test = test_data3[['w2v_features']]\n",
    "features_test_w2v = np.array(list(map(np.array, features_test.w2v_features)))\n",
    "\n",
    "\n",
    "label_test = np.array(test_data3.loc[:, first_reviewer: last_reviewer])\n",
    "save_array(label_test, 'test_label.npy')\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "params = [\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "           'learning_rate_init': 0.005, 'max_iter': 20000,'hidden_layer_sizes': (32)},\n",
    "    \n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "           'learning_rate_init': 0.01, 'max_iter': 10000,'hidden_layer_sizes': (32,16)},\n",
    "    \n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "           'learning_rate_init': 0.01, 'max_iter': 10000,'hidden_layer_sizes': (64,32)},\n",
    "    \n",
    "          #{'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "          # 'learning_rate_init': 0.01, 'max_iter': 10000,'hidden_layer_sizes': (16, 8)},\n",
    "    \n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "           'learning_rate_init': 0.001, 'max_iter': 10000,'hidden_layer_sizes': (8,)},\n",
    "    \n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "           'learning_rate_init': 0.001, 'max_iter': 10000,'hidden_layer_sizes': (16,)}\n",
    "    \n",
    "          #{'solver': 'sgd', 'learning_rate': 'constant', 'alpha': 1e-3,'validation_fraction':0.3,\n",
    "          # 'learning_rate_init': 0.001, 'max_iter': 10000,'hidden_layer_sizes': (4,)}\n",
    "          #{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "          # 'learning_rate_init': 0.001, 'max_iter': 10000, 'hidden_layer_sizes': (8,)},\n",
    "          #{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "          #'learning_rate_init': 0.001, 'max_iter': 10000, 'hidden_layer_sizes': (4,)},\n",
    "          #{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "          # 'learning_rate_init': 0.001, 'max_iter': 10000, 'hidden_layer_sizes': (8-4,)},\n",
    "          ]\n",
    "#labels = [\"hidden64\", \"hidden32\", \"hidden16\", \"hidden8\", \"hidden4\", \"hidden8-4\"]\n",
    "labels = [\"hidden32\", \"hidden32-16\", \"hidden64-32\", \"hidden8\", \"hidden16\" ]\n",
    "mlps = []\n",
    "for label, param in zip(labels, params):\n",
    "    print(\"training: %s\" % label)\n",
    "    mlp = MLPClassifier(verbose=0, random_state=1, n_iter_no_change=100, **param)\n",
    "    mlp.fit(features_train_w2v, label_train)\n",
    "    np.save('D:\\\\reviewer recommendation\\\\LDA&w2v\\\\score_test\\\\score_test%s_%s.npy' % (vector_size, label), mlp.predict_proba(features_test_w2v))\n",
    "    np.save('D:\\\\reviewer recommendation\\\\LDA&w2v\\\\score_train\\\\score_train%s_%s.npy' % (vector_size, label), mlp.predict_proba(features_train_w2v))\n",
    "    mlps.append(mlp)\n",
    "   #print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "   #print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "for mlp, label in zip(mlps, labels):\n",
    "        ax.plot(mlp.loss_curve_, label=label)\n",
    "plt.legend(loc=1)\n",
    "plt.show()\n",
    "\n",
    "#mlp = MLPClassifier(activation='logistic', solver='sgd', max_iter=10000, alpha=1e-03, hidden_layer_sizes=(8,))\n",
    "#mlp.fit(features_train_w2v, label_train)\n",
    "# clf.predict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('store_ranks.npy', 'rb') as reader:\n",
    "    ranks3 = pickle.load(reader)\n",
    "collections.Counter(ranks3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visulization experiment result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.utils.fixes import signature\n",
    "def draw_precision_recall_curve(score, label):\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "\n",
    "    Y_test = np.load(label)\n",
    "    Y_score = np.load(score)\n",
    "    print(Y_test.shape)\n",
    "    print(Y_score.shape)\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(), Y_score.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(Y_test, Y_score,\n",
    "                                                     average=\"micro\")\n",
    "    print('Average precision score, micro-averaged over all classes: {0:0.2f}' .format(average_precision[\"micro\"]))\n",
    "    plt.figure()\n",
    "    plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2, where='post')\n",
    "    plt.step(recall['micro'], precision['micro'], color='g', alpha=0.2, where='post')\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "    plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='b', **step_kwargs)\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title( 'Average precision score, micro-averaged over all classes: AP={0:0.2f}' .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the df_probs_train.npy and *_test.npy files from server\n",
    "draw_precision_recall_curve('df_probs_train.npy', 'train_label.npy')\n",
    "draw_precision_recall_curve('score_test.npy', 'test_label.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "best_params = {'n_estimators':100, 'max_depth':3, 'min_samples_split':2}\n",
    "cv_results_ = {\n",
    "'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
    "'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
    "'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
    "'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
    "'rank_test_score'    : [2, 4, 3, 1],\n",
    "'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
    "'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
    "'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
    "'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
    "'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
    "'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
    "'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
    "'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
    "'params'             : [{'kernel': 'poly', 'degree': 2}],\n",
    "}\n",
    "means = cv_results_['mean_test_score']\n",
    "stds = cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "\n",
    "with open('gridsearch.txt','w') as file:\n",
    "    file.write('best params\\n')\n",
    "    file.write(json.dumps(best_params))\n",
    "    file.write('\\nmean_test_score\\n')\n",
    "    file.write(json.dumps(cv_results_['mean_test_score']))\n",
    "    file.write('\\nstd_test_score\\n')\n",
    "    file.write(json.dumps(cv_results_['std_test_score']))\n",
    "    file.write('\\nmean_train_score\\n')\n",
    "    file.write(json.dumps(cv_results_['mean_train_score']))\n",
    "    file.write('\\nstd_train_score\\n')\n",
    "    file.write(json.dumps(cv_results_['std_train_score']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_resampled[label_train_resampled.sum(axis=1)!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model and test its performance\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "model.fit(features_train_w2v, label_train)\n",
    "\n",
    "#for x, y in zip(features_test_w2v[0:20,], label_test[0:20,]):\n",
    "#    print(x)\n",
    "#    print(y)\n",
    "#    y_pred = model.predict(x.reshape(1, -1))\n",
    "#    print(y_pred[0])\n",
    "#    precision = precision_score(y, y_pred[0])\n",
    "#    recall = recall_score(y, y_pred[0])\n",
    "#    print(precision)\n",
    "#    print(recall)\n",
    "df_probs = model.predict_proba(features_test_w2v)\n",
    "print(df_probs.shape)\n",
    "print(average_precision_score(label_test, df_probs, average='weighted'))\n",
    "\n",
    "for i in range(1,10):\n",
    "    result = pd.DataFrame({'Precision' :[], 'Recall' :[]}, dtype=np.int64)\n",
    "    \n",
    "    \n",
    "    for probs, y in zip(df_probs, label_test):\n",
    "        y_pred = np.zeros_like(probs, dtype=np.int64)\n",
    "    \n",
    "        y_pred[probs.argsort()[-i:]] = 1\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        #print(\"Training Precision = \", precision)\n",
    "        #print(\"Training Recall = \", recall)\n",
    "        result = result.append({'Precision':precision, 'Recall':recall}, ignore_index=True)\n",
    "    filename = 'D:\\\\reviewer recommendation\\\\LDA&w2v\\\\result\\\\2080split%dt.csv' % i\n",
    "    result.to_csv(filename)\n",
    "    \n",
    "df_probs = model.predict_proba(features_train_w2v)\n",
    "print(df_probs.shape)\n",
    "print(average_precision_score(label_train, df_probs, average='weighted'))\n",
    "\n",
    "for i in range(1,10):\n",
    "    result = pd.DataFrame({'Precision' :[], 'Recall' :[]}, dtype=np.int64)\n",
    "    \n",
    "    \n",
    "    for probs, y in zip(df_probs, label_train):\n",
    "        y_pred = np.zeros_like(probs, dtype=np.int64)\n",
    "    \n",
    "        y_pred[probs.argsort()[-i:]] = 1\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        #print(\"Training Precision = \", precision)\n",
    "        #print(\"Training Recall = \", recall)\n",
    "        result = result.append({'Precision':precision, 'Recall':recall}, ignore_index=True)\n",
    "    filename = 'D:\\\\reviewer recommendation\\\\LDA&w2v\\\\result\\\\2080split%d.csv' % i\n",
    "    result.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('numpy.npy', df_probs)\n",
    "d = np.load('numpy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(i, n=1):\n",
    "    \n",
    "    column_names = test_data3.loc[:,first_reviewer:last_reviewer].columns.values\n",
    "    \n",
    "    labels = np.array(test_data3.iloc[i].loc[first_reviewer:last_reviewer])\n",
    "    print('true:' + column_names[labels==1])\n",
    "\n",
    "    print('prediction:' + column_names[df_probs[i].argsort()[-n:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    show_result(i,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data3[train_data3['jaredpar']==1].shape)\n",
    "print(train_data3[train_data3['jasonmalinowski']==1].shape)\n",
    "print(train_data3[train_data3['Pilchie']==1].shape)\n",
    "print(train_data3[train_data3['CyrusNajmabadi']==1].shape)\n",
    "print(train_data3[train_data3['VSadov']==1].shape)\n",
    "print(train_data3[train_data3['cston']==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision = []\n",
    "Recall = []\n",
    "for i in range(1,10):\n",
    "    filename = 'D:\\\\reviewer recommendation\\\\LDA&w2v\\\\result\\\\2080split%d.csv' % i\n",
    "\n",
    "    result = pd.read_csv(filename)\n",
    "    #del result['Unnamed: 0']\n",
    "    Precision.append(result['Precision'].mean())\n",
    "    Recall.append(result['Recall'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(data, label):\n",
    "    plt.plot(data, '-', lw=2, label=label)\n",
    "    plt.xlabel('# of reviewers')\n",
    "    plt.ylim(0,1)\n",
    "    plt.ylabel('average value')\n",
    "    plt.legend(prop={'size': 12}, loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_line(Precision, 'Precision')\n",
    "draw_line(Recall, 'Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.plot(range(10), range(10), \"o\")\n",
    "plt.show()\n",
    "\n",
    "f.savefig(\"foo.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "#features_train_w2v_resampled, label_train_resampled = ros.fit_resample(features_train_w2v, label_train)\n",
    "features_train_w2v_resampled, label_train_resampled = SMOTE().fit_resample(features_train_w2v, label_train)\n",
    "#features_train_w2v_resampled, label_train_resampled = ADASYN().fit_resample(features_train_w2v, label_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []\n",
    "for i in range(0, label_train.shape[1]):\n",
    "    count_dict = Counter(label_train[:,i])\n",
    "    count_0 = count_dict[0]\n",
    "    count_1 = count_dict[1]\n",
    "    count_dict = dict()\n",
    "    count_dict[0] = count_1\n",
    "    count_dict[1] = count_0\n",
    "    list_dict.append(count_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
