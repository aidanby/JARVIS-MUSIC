{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# languange processing imports\n",
    "#import nltk\n",
    "import string\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pprint import pprint\n",
    "# model imports\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import CoherenceModel, Phrases, phrases\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read tables from mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printable = set(string.printable)\n",
    "s = 'Ti\\u00ebsto'\n",
    "list(filter(lambda x: x in printable, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lyrics.csv\n",
    "data = pd.read_csv('lyrics.csv', names=['singer', 'song', 'lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        return simple_preprocess(text)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# searching using d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lyrics['singer'] = list(map(simple_preprocess, lyrics.singer))\n",
    "#lyrics['song'] = list(map(simple_preprocess, lyrics.singer))\n",
    "#data['lyrics'] = list(map(preprocess, data.lyrics.values))\n",
    "data = pd.read_csv('lyrics.csv', names = ['singer', 'song', 'lyrics'])\n",
    "data = data[~data['lyrics'].isna()]\n",
    "data = data[~data['song'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_w2v_features(d2v_model, sentence):\n",
    "    #vector = d2v_model.docvecs[i]\n",
    "        \n",
    "    inferred_vector = d2v_model.infer_vector(sentence)\n",
    "    return inferred_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric = 'Im a shattered  yeah  Pride and joy and greed and sex Thats what makes our ton the best Pride and joy and dirty dreams'\n",
    "inferred_vector = model.infer_vector(lyric.split(' '))\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "rank = [[data.index.get_loc(int(docid.strip('doc'))), sim] for docid, sim in sims]\n",
    "#rank = [((int(docid.strip('doc'))), sim) for docid, sim in sims]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df = pd.DataFrame(rank, columns=['docid', 'sim'])\n",
    "rank_df = rank_df.sort_values(by=['docid'])\n",
    "rank_df.index = rank_df.docid\n",
    "rank_df = rank_df.drop(columns=['docid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df + matrix_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_rank = np.array(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_rank.sort(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_rank\n",
    "#array_rank[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('store_ranks.npy', 'wb') as writer:\n",
    "    pickle.dump(ranks, writer)\n",
    "    \n",
    "with open('store_ranks.npy', 'rb') as reader:\n",
    "    ranks3 = pickle.load(reader)\n",
    "collections.Counter(ranks3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# searching song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# languange processing imports\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy\n",
    "#import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    #stop_words = stopwords.words('english')\n",
    "    #return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    #return [[word for word in doc.split(' ') if word not in stop_words] for doc in texts]\n",
    "    return [[word for word in doc.split(' ')] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# searching songs using lyrics ,song name and d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what the boy\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def filter_out_words(sentence):\n",
    "    base_url = 'https://ZyFly.lib.id/lyricsMatch@dev/'\n",
    "    params = {'sentence': sentence}\n",
    "    response = requests.get(base_url, params)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lyrics.csv\n",
    "def process_corpus(column):\n",
    "    data = pd.read_csv('lyrics.csv', names=['singer', 'song', 'lyrics'])\n",
    "    data = data[~data['lyrics'].isna()]\n",
    "    data['lyrics'] = list(map(filter_out_words, data.lyrics))\n",
    "    \n",
    "    #data_words_nostops = remove_stopwords(data.lyrics)\n",
    "    #data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    #corpus = list(map(lambda words: \" \".join(words), data_lemmatized))\n",
    "    \n",
    "    count_vect = CountVectorizer().fit(data[column].values)\n",
    "    words_count = count_vect.transform(data[column].values)\n",
    "    tf_transformer = TfidfTransformer(sublinear_tf=True).fit(words_count)\n",
    "    words_tfidf = tf_transformer.transform(words_count)\n",
    "    return data, count_vect, tf_transformer, words_tfidf\n",
    "    #scipy.sparse.save_npz('sparse_matrix.npz',words_tfidf) \n",
    "    #words_tfidf = scipy.sparse.load_npz('sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(count_vect, tf_transformer, lyric):\n",
    "    #lyric_no_stops = remove_stopwords(lyric)\n",
    "    #lyric_lemmatized = lemmatization(lyric_no_stops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    ##lyric_lemmatized = lemmatize_more(lyric_lemmatized)\n",
    "    #lyric = list(map(lambda words: \" \".join(words), lyric_lemmatized))\n",
    "    words_count = count_vect.transform(lyric)\n",
    "    search_tfidf = tf_transformer.transform(words_count)\n",
    "    return search_tfidf\n",
    "\n",
    "def search_top5(words_tfidf, search_tfidf, data):  \n",
    "    matrix_dot = np.dot(words_tfidf, search_tfidf[0].transpose())\n",
    "    #print(type(matrix_dot.todense()))\n",
    "    matrix_dot = matrix_dot.todense()\n",
    "    #sorted_index = matrix_dot.argsort(axis=0)\n",
    "    #sorted_index = np.array(sorted_index)[:-6:-1]\n",
    "    return matrix_dot\n",
    "    #for iindex in sorted_index:\n",
    "    #    print(data.iloc[iindex].song)\n",
    "    #print(sorted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count_vect_lyric, tf_transformer_lyric, words_tfidf_lyric = process_corpus('lyrics')\n",
    "data, count_vect_name, tf_transformer_name, words_tfidf_name = process_corpus('song')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lyric = ['But I am much too full of resentment']\n",
    "#lyric = ['I fall apart down to my core surprised when you caught me off my guard']\n",
    "#lyric = ['She fooled me twice and its all my fault yeah She cut too deep  now she left me scarred']\n",
    "#lyric = ['resentment']\n",
    "#lyric = ['my fault']\n",
    "#lyric = ['Ive been shattered My brains been battered']\n",
    "lyric = ['Overwhelming Everything about you is so overwhelming']\n",
    "#lyric = ['I think I got one Her soul is presidential like Barack']\n",
    "#lyric = ['To the left  to the left To the left  to the left']\n",
    "search_tfidf_lyric = process_query(count_vect_lyric, tf_transformer_lyric, lyric)\n",
    "matrix_lyric = search_top5(words_tfidf_lyric, search_tfidf_lyric, data)\n",
    "\n",
    "search_tfidf_name = process_query(count_vect_name, tf_transformer_name, lyric)\n",
    "matrix_name = search_top5(words_tfidf_name, search_tfidf_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_vector = model.infer_vector(lyric[0].split(' '))\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "rank = [[data.index.get_loc(int(docid.strip('doc'))), sim] for docid, sim in sims]\n",
    "\n",
    "rank_df = pd.DataFrame(rank, columns=['docid', 'sim'])\n",
    "rank_df = rank_df.sort_values(by=['docid'])\n",
    "rank_df.index = rank_df.docid\n",
    "rank_df = rank_df.drop(columns=['docid'])\n",
    "\n",
    "matrix_d2v = rank_df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/results?search_query=overwhelming+jon+bellion\n",
      "https://www.youtube.com/results?search_query=lay+me+down+-+single+version+sam+smith\n",
      "https://www.youtube.com/results?search_query=lay+me+down+sam+smith\n",
      "https://www.youtube.com/results?search_query=hand+of+god+(outro)+jon+bellion\n",
      "https://www.youtube.com/results?search_query=betray+and+degrade+seether\n"
     ]
    }
   ],
   "source": [
    "# output is in a dict. 5 items. key varies but always different. each item is a list. list[0] sone name, list[1] singer name, list[2] youtube link\n",
    "matrix_dot = 0.7*matrix_lyric + 0.1*matrix_name + 0.2*matrix_d2v\n",
    "sorted_index = matrix_dot.argsort(axis=0)\n",
    "sorted_index = np.array(sorted_index)[:-6:-1]\n",
    "result_dict = dict()\n",
    "link = 'https://www.youtube.com/results?search_query='\n",
    "for iindex in sorted_index:\n",
    "    parameter = data.iloc[iindex].song.values[0].replace(' ', '+') + '+' + data.iloc[iindex].singer.values[0].replace(' ', '+')\n",
    "    print(link+parameter.lower())\n",
    "    result = []\n",
    "    result.append(data.iloc[iindex].song.values[0])\n",
    "    result.append(data.iloc[iindex].singer.values[0])\n",
    "    result.append(link+parameter.lower())\n",
    "    result_dict[iindex[0]] = result\n",
    "    \n",
    "#    print(data.iloc[iindex].song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_lyric[np.isnan(matrix_lyric)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    #return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    return [[word for word in doc.split(' ') if word not in stop_words] for doc in texts]\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "#def lemmatize_more(data):\n",
    "#    tt = []\n",
    "#    for tokens in data:\n",
    "#        dd = [get_lemma(token) for token in tokens if len(token) > 4]\n",
    "#        tt.append(dd)\n",
    "#    return tt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "import spacy\n",
    "data_words_nostops = remove_stopwords(data.lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lemmatized = lemmatize_more(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(map(lambda words: \" \".join(words), data_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer().fit(data.lyrics.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = count_vect.transform(data.lyrics.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(sublinear_tf=True).fit(words_count)\n",
    "words_tfidf = tf_transformer.transform(words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.sparse.save_npz('sparse_matrix.npz',words_tfidf) \n",
    "words_tfidf = scipy.sparse.load_npz('sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lyric = ['I fall apart down to my core surprised when you caught me off my guard']\n",
    "#lyric = ['She fooled me twice and its all my fault yeah She cut too deep  now she left me scarred']\n",
    "#lyric = ['Ive been shattered My brains been battered']\n",
    "#lyric = ['Overwhelming Everything about you is so overwhelming']\n",
    "#lyric = ['I think I got one Her soul is presidential like Barack']\n",
    "lyric = ['But I am much too full of resentment']\n",
    "#lyric_no_stops = remove_stopwords(lyric)\n",
    "#lyric_lemmatized = lemmatization(lyric_no_stops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "#lyric_lemmatized = lemmatize_more(lyric_lemmatized)\n",
    "#lyric = list(map(lambda words: \" \".join(words), lyric_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = count_vect.transform(lyric)\n",
    "search_tfidf = tf_transformer.transform(words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_result = pd.DataFrame(index=data.index, columns=['sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(np.dot(words_tfidf, search_tfidf[0].transpose()))\n",
    "#print(words_tfidf.shape)\n",
    "#print(search_tfidf[0].transpose().shape)\n",
    "\n",
    "matrix_dot = np.dot(words_tfidf, search_tfidf[0].transpose())\n",
    "#print(matrix_dot.shape)\n",
    "print(type(matrix_dot.todense()))\n",
    "matrix_dot = matrix_dot.todense()\n",
    "sorted_index = matrix_dot.argsort(axis=0)\n",
    "sorted_index = np.array(sorted_index)[:-6:-1]\n",
    "print(sorted_index)\n",
    "#print(matrix_dot[6376])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[sorted_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pr_index, pr in data.iloc[:10000,].iterrows():\n",
    "for pr_index, pr in data.iterrows():\n",
    "    reviewed_iloc = data.index.get_loc(pr_index)\n",
    "    #print(reviewed_iloc)\n",
    "    if len(np.dot(search_tfidf[0], words_tfidf[reviewed_iloc].transpose()).tocoo().data) == 1: \n",
    "        similar_score = np.dot(search_tfidf[0], words_tfidf[reviewed_iloc].transpose()).tocoo().data[0] #if len(np.dot(words_tfidf[pr_iloc], words_tfidf[i].transpose()).tocoo().data) == 1]\n",
    "        similar_result.loc[pr_index, 'sim'] = similar_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similar_result.sort_values(by=['sim'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,4], [3,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lyrics.csv', names=['singer', 'song', 'lyrics'])\n",
    "data = data[~data['lyrics'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_index = data.index.get_loc(6383)\n",
    "print(s_index)\n",
    "print(data.iloc[s_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_result.loc[6399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_result[similar_result['sim'].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# song name search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer().fit(corpus)\n",
    "words_count = count_vect.transform(corpus)\n",
    "tf_transformer = TfidfTransformer(sublinear_tf=True).fit(words_count)\n",
    "words_tfidf = tf_transformer.transform(words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying fuzzy string comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n",
    "process.extract(\"new york jets\", choices, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[6343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\"Im a shattered  yeah  Pride and joy and greed and sex Thats what makes our town the best Pride and joy and dirty dreams\", 'Shattered  shattered Love and hope and sex and dreams Are still surviving on the street Look at me  Im in tatters Im a shattered Shattered  Friends are so alarming And my lovers never charming Lifes just a cocktail party on the street Big Apple People dressed in plastic bags Directing traffic Some kind of fashion Shattered  Laughter  joy  and loneliness and sex and sex and sex and sex Look at me  Im in tatters Im a shattered Shattered  All this chitter-chatter  chitter-chatter  chitter-chatter about Shmatta  shmatta  shmatta  I cant give it away on 7th Avenue This towns been wearing tatters shattered  shattered Work and work for love and sex Aint you hungry for success  success  success  success Does it matter? Shattered Does it matter? Im shattered Shattered  Ahhh  look at me  Im a shattered Im a shattered Look at me  Im a shattered  yeah  Pride and joy and greed and sex Thats what makes our town the best Pride and joy and dirty dreams and still surviving on the street And look at me  Im in tatters  yeah Ive been battered  what does it matter Does it matter  uh-huh Does it matter  uh-huh  Im a shattered Dont you know the crime rate is going up  up  up  up  up To live in this town you must be tough  tough  tough  tough  tough! You got rats on the west side Bed bugs uptown What a mess this towns in tatters Ive been shattered My brains been battered  splattered all over Manhattan  Uh-huh  this towns full of money grabbers Go ahead  bite the Big Apple  dont mind the maggots  huh Shadoobie  my brains been battered My friends they come around they Flatter  flatter  flatter  flatter  flatter  flatter  flatter Pile it up  pile it high on the platter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "result = regex.match('(Im a yeah  Pride and joy and greed and sex Thats what makes our town the best Pride and joy and dirty dreams){e<=1}', 'Shattered  shattered Love and hope and sex and dreams Are still surviving on the street Look at me  Im in tatters Im a shattered Shattered  Friends are so alarming And my lovers never charming Lifes just a cocktail party on the street Big Apple People dressed in plastic bags Directing traffic Some kind of fashion Shattered  Laughter  joy  and loneliness and sex and sex and sex and sex Look at me  Im in tatters Im a shattered Shattered  All this chitter-chatter  chitter-chatter  chitter-chatter about Shmatta  shmatta  shmatta  I cant give it away on 7th Avenue This towns been wearing tatters shattered  shattered Work and work for love and sex Aint you hungry for success  success  success  success Does it matter? Shattered Does it matter? Im shattered Shattered  Ahhh  look at me  Im a shattered Im a shattered Look at me  Im a shattered  yeah  Pride and joy and greed and sex Thats what makes our town the best Pride and joy and dirty dreams and still surviving on the street And look at me  Im in tatters  yeah Ive been battered  what does it matter Does it matter  uh-huh Does it matter  uh-huh  Im a shattered Dont you know the crime rate is going up  up  up  up  up To live in this town you must be tough  tough  tough  tough  tough! You got rats on the west side Bed bugs uptown What a mess this towns in tatters Ive been shattered My brains been battered  splattered all over Manhattan  Uh-huh  this towns full of money grabbers Go ahead  bite the Big Apple  dont mind the maggots  huh Shadoobie  my brains been battered My friends they come around they Flatter  flatter  flatter  flatter  flatter  flatter  flatter Pile it up  pile it high on the platter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = data.lyrics.values\n",
    "%time process.extract(\"Im a shattered  yeah  Pride and joy and greed and sex Thats what makes our town the best Pride and joy and dirty dreams\", choices, limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(reviewers_semantic_sim['rchande'].isnull())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
